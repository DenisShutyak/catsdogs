{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import  plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from PIL import Image, ImageDraw\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import keras.callbacks\n",
    "import tensorflow.keras.losses\n",
    "import os\n",
    "import pydot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Parameters\n",
    "N_CLASSES = 2 # total number of classes\n",
    "IMG_HEIGHT = 448 # the image height to be resized to\n",
    "IMG_WIDTH = 448 # the image width to be resized to\n",
    "CHANNELS = 3 # The 3 color channels, change to 1 if grayscale\n",
    "initial_train_data = r'C:\\Users\\densh\\Desktop\\catdogs\\dataset\\augmented_data'\n",
    "initial_validation = r'C:\\Users\\densh\\Desktop\\catdogs\\dataset\\valid'\n",
    "tf_train_data = r'C:\\Users\\densh\\Desktop\\catdogs\\dataset\\augmented_data_tf'\n",
    "tf_train = r'C:\\Users\\densh\\Desktop\\catdogs\\dataset\\train_tf'\n",
    "tf_validation = r'C:\\Users\\densh\\Desktop\\catdogs\\dataset\\valid_tf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment to plot nn graph\n",
    "os.environ[\"PATH\"] += os.pathsep + r'C:\\Program Files\\Graphviz\\bin'\n",
    "os.environ[\"PATH\"] += os.pathsep + r'C:\\Users\\densh\\AppData\\Roaming\\Python\\Python38\\Scripts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "def parse_dataset(dataset_path, ext='jpg'):\n",
    "    \"\"\"\n",
    "    Used to extract information about our dataset. It does iterate over all images and return a DataFrame with\n",
    "    the data (age, gender and sex) of all files.\n",
    "    \"\"\"\n",
    "    def parse_info_from_file(path):\n",
    "        \"\"\"\n",
    "        Parse information from a single file\n",
    "        \"\"\"\n",
    "        try:\n",
    "            title, ext = os.path.splitext(os.path.basename(path))\n",
    "            txt_file_name = title + '.txt'\n",
    "            with open(os.path.join(os.path.dirname(os.path.abspath(path)),txt_file_name),'r') as old_txt:\n",
    "                labelList = old_txt.readlines()\n",
    "                for label in labelList:\n",
    "                    label = label.strip().split()\n",
    "                    class_= int(label[1])\n",
    "                    x1    = float(label[2])\n",
    "                    y1    = float(label[3])\n",
    "                    x2    = float(label[4])\n",
    "                    y2    = float(label[5])\n",
    "                    \n",
    "                    w = x2 - x1\n",
    "                    h = y2 - y1\n",
    "                    x1 = x1+w/2\n",
    "                    y1 = y1+h/2\n",
    "                    x2=w\n",
    "                    y2=h\n",
    "            return class_, x1, y1, x2, y2\n",
    "        except Exception as ex:\n",
    "            return class_, None, None, None, None\n",
    "        \n",
    "    files = glob.glob(os.path.join(dataset_path, \"*.%s\" % ext))\n",
    "    \n",
    "    records = []\n",
    "    for file in files:\n",
    "        info = parse_info_from_file(file)\n",
    "        records.append(info)\n",
    "        \n",
    "    df = pd.DataFrame(records)\n",
    "    df['file'] = files\n",
    "    df.columns = ['class', 'x1', 'y1', 'x2', 'y2', 'file']\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = parse_dataset(tf_train)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = parse_dataset(tf_validation)\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator():\n",
    "    \"\"\"\n",
    "    Data generator for the UTKFace dataset. This class should be used when training our Keras multi-output model.\n",
    "    \"\"\"\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "    def generate_split_indexes(self):\n",
    "        p = np.random.permutation(len(self.df))\n",
    "        train_idx = p[:]\n",
    "        \n",
    "        return train_idx\n",
    "    \n",
    "    def generate_test_split_indexes(self,length):\n",
    "        p = np.random.randint(0,len(self.df),length)\n",
    "        train_idx = p[:]\n",
    "        \n",
    "        return train_idx\n",
    "    \n",
    "    def preprocess_image(self, img_path):\n",
    "        \"\"\"\n",
    "        Used to perform some minor preprocessing on the image before inputting into the network.\n",
    "        \"\"\"\n",
    "        \n",
    "        im = Image.open(img_path)\n",
    "        im = im.resize((IMG_WIDTH, IMG_HEIGHT))\n",
    "        im = np.array(im) / 255.0\n",
    "        return im\n",
    "        \n",
    "    def generate_images(self, image_idx, is_training, batch_size):\n",
    "        \"\"\"\n",
    "        Used to generate a batch with images when training/testing/validating our Keras model.\n",
    "        \"\"\"\n",
    "        \n",
    "        # arrays to store our batched data\n",
    "        images, classes, x1, y1, x2, y2 = [], [], [], [], [], []\n",
    "        while True:\n",
    "            for idx in image_idx:\n",
    "                imag = self.df.iloc[idx]\n",
    "                \n",
    "                classes_ = imag['class']\n",
    "                x1_ = imag['x1']\n",
    "                y1_ = imag['y1']\n",
    "                x2_ = imag['x2']\n",
    "                y2_ = imag['y2']\n",
    "                file = imag['file']\n",
    "                im = self.preprocess_image(file)\n",
    "                \n",
    "                classes.append(classes_)\n",
    "                x1.append(x1_)\n",
    "                y1.append(y1_)\n",
    "                x2.append(x2_)\n",
    "                y2.append(y2_)\n",
    "                images.append(im)\n",
    "                # yielding condition\n",
    "                if len(images) >= batch_size:\n",
    "                    yield np.array(images), [np.array(to_categorical(classes,2)),np.array(x1),np.array(y1), np.array(x2), np.array(y2)]\n",
    "                    images, classes, x1, y1, x2, y2 = [], [], [], [], [], []\n",
    "                    \n",
    "            if not is_training:\n",
    "                break\n",
    "                \n",
    "data_generator = DataGenerator(df)\n",
    "train_idx = data_generator.generate_split_indexes() \n",
    "v_data_generator = DataGenerator(val_df)\n",
    "valid_idx = v_data_generator.generate_split_indexes() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.applications\n",
    "\n",
    "inputs = keras.Input(shape=(IMG_WIDTH, IMG_HEIGHT, CHANNELS), name=\"img_input\")\n",
    "\n",
    "N_mobile = tensorflow.keras.applications.MobileNetV2(input_shape=(IMG_WIDTH, IMG_HEIGHT, CHANNELS), include_top=False, weights='imagenet', alpha = 0.35)#, weights='imagenet')\n",
    "N_mobile.trainable = False\n",
    "base_model_output = N_mobile.layers[-1].output\n",
    "#class_output = layers.Conv2D(256, (2,2), activation=\"relu\")(base_model_output)\n",
    "#d = layers.Conv2D(1024, (2,2), activation=\"relu\")(base_model_output)\n",
    "class_output = layers.GlobalAveragePooling2D()(base_model_output)\n",
    "class_output = layers.BatchNormalization()(class_output)\n",
    "class_output = layers.Dense(2, activation=\"softmax\",name=\"class_output\")(class_output)\n",
    "\n",
    "x = layers.Conv2D(64, kernel_size=3)(base_model_output)\n",
    "#x = layers.Conv2D(4, activation=\"sigmoid\", kernel_size=3)(x)\n",
    "x = layers.Conv2D(4, kernel_size=3)(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Flatten()(x)\n",
    "box_output1 = layers.Dense(1, activation=\"linear\", name= \"Box_output1\")(x)\n",
    "\n",
    "x = layers.Conv2D(64, kernel_size=3)(base_model_output)\n",
    "#x = layers.Conv2D(4, activation=\"sigmoid\", kernel_size=3)(x)\n",
    "x = layers.Conv2D(4, kernel_size=3)(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Flatten()(x)\n",
    "box_output2 = layers.Dense(1, activation=\"linear\", name= \"Box_output2\")(x)\n",
    "\n",
    "x = layers.Conv2D(64, kernel_size=3)(base_model_output)\n",
    "#x = layers.Conv2D(4, activation=\"sigmoid\", kernel_size=3)(x)\n",
    "x = layers.Conv2D(2, kernel_size=3)(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Flatten()(x)\n",
    "box_output3 = layers.Dense(1, activation=\"relu\", name= \"Box_output3\")(x)\n",
    "\n",
    "x = layers.Conv2D(64, kernel_size=3)(base_model_output)\n",
    "#x = layers.Conv2D(4, activation=\"sigmoid\", kernel_size=3)(x)\n",
    "x = layers.Conv2D(2, kernel_size=3)(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Flatten()(x)\n",
    "box_output4 = layers.Dense(1, activation=\"relu\", name= \"Box_output4\")(x)\n",
    "\n",
    "model2 = keras.Model(inputs=N_mobile.input, outputs = [class_output,box_output1,box_output2,box_output3,box_output4], name=\"cats_dogs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model2,to_file='model.pdf',show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_mse(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.math.log1p(tf.math.squared_difference(y_pred, y_true)), axis=-1)\n",
    "\n",
    "def focal_loss(alpha=0.9, gamma=2):\n",
    "  def focal_loss_with_logits(logits, targets, alpha, gamma, y_pred):\n",
    "    weight_a = alpha * (1 - y_pred) ** gamma * targets\n",
    "    weight_b = (1 - alpha) * y_pred ** gamma * (1 - targets)\n",
    "    \n",
    "    return (tf.math.log1p(tf.exp(-tf.abs(logits))) + tf.nn.relu(-logits)) * (weight_a + weight_b) + logits * weight_b\n",
    "\n",
    "  def loss(y_true, y_pred):\n",
    "    y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "    logits = tf.math.log(y_pred / (1 - y_pred))\n",
    "\n",
    "    loss = focal_loss_with_logits(logits=logits, targets=y_true, alpha=alpha, gamma=gamma, y_pred=y_pred)\n",
    "\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_lr = 1e-2\n",
    "epochs = 50\n",
    "model2.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(lr=1e-2, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False),#SGD(lr = 1e-2),#Adam(lr=init_lr,beta_1=0.9),\n",
    "     loss={\n",
    "                  'Box_output1': log_mse,\n",
    "                  'Box_output2': log_mse,\n",
    "                  'Box_output3': log_mse,\n",
    "                  'Box_output4': log_mse,\n",
    "                  'class_output': focal_loss()},\n",
    "     loss_weights={\n",
    "                  'Box_output1': 1,\n",
    "                  'Box_output2': 1,\n",
    "                  'Box_output3': 1,\n",
    "                  'Box_output4': 1,\n",
    "                  'class_output': 1},\n",
    "     metrics={\n",
    "                  \"Box_output1\":  'mse',\n",
    "                  \"Box_output2\":  'mse',\n",
    "                  \"Box_output3\":  'mse',\n",
    "                  \"Box_output4\":  'mse',\n",
    "                  \"class_output\": 'accuracy',\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch>20:\n",
    "        return lr*0.9\n",
    "    else:\n",
    "        return lr\n",
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=3, min_lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "valid_batch_size = 64\n",
    "train_gen =   data_generator.generate_images(train_idx, is_training=True, batch_size=batch_size)\n",
    "valid_gen = v_data_generator.generate_images(valid_idx, is_training=True, batch_size=valid_batch_size)\n",
    "callbacks = [\n",
    "    #ModelCheckpoint(r\"C:\\Users\\densh\\Desktop\\catdogs\\keras_try\\Checkpoint\", monitor='val_loss', save_best_only=True), \n",
    "    reduce_lr, callback,\n",
    "    keras.callbacks.TensorBoard(\n",
    "    log_dir=r\"C:\\Users\\densh\\Desktop\\catdogs\\keras_try\\log\",write_images=True,\n",
    "    histogram_freq=0,  # How often to log histogram visualizations\n",
    "    embeddings_freq=0,  # How often to log embedding visualizations\n",
    "    update_freq=\"epoch\")  # How often to write logs (default: once per epoch)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "model2.fit(train_gen,steps_per_epoch=len(train_idx)//(1*batch_size),  epochs=100, callbacks=callbacks, shuffle=True, validation_data=valid_gen, validation_steps=len(valid_idx)//valid_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save(r\"C:\\Users\\densh\\Desktop\\catdogs\\keras_try\\model_testing\\model5hw\", overwrite=True, include_optimizer=False, save_format='tf', signatures=None, options=None, save_traces=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = v_data_generator.generate_test_split_indexes(10)\n",
    "test_gen  = v_data_generator.generate_images(train_idx, is_training=False, batch_size=1)\n",
    "c = model2.predict(test_gen,steps=len(train_idx))\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=0\n",
    "df2 = val_df.iloc[train_idx,]\n",
    "df2.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "# fontScale\n",
    "fontScale = 1\n",
    "# Blue color in BGR\n",
    "color = (255, 0, 0)\n",
    "# Line thickness of 2 px\n",
    "thickness = 2 \n",
    "# Using cv2.putText() method\n",
    "i=0\n",
    "for idx in train_idx:\n",
    "    file = val_df.iloc[int(idx)]\n",
    "    for image in glob.glob(os.path.join(file['file'])):\n",
    "        title, ext = os.path.splitext(os.path.basename(image))\n",
    "        txt_file_name = title + '.txt'\n",
    "        cv_image = cv2.imread(image)\n",
    "        H, W, _ = cv_image.shape\n",
    "        #top_left = (int(c[1][i]*W),int(c[2][i]*H))\n",
    "        top_left = (int((c[1][i]-c[3][i]/2)*W),int((c[2][i]+c[4][i]/2)*H))\n",
    "        #bottom_right = (int(c[3][i]*W),int(c[4][i]*H))\n",
    "        bottom_right = (int((c[1][i]+c[3][i]/2)*W),int((c[2][i]-c[4][i]/2)*H))\n",
    "        image = cv2.rectangle(cv_image, top_left, bottom_right, [0, 0, 0], 2)\n",
    "        \n",
    "        top_left = (int((df2['x1'][idx]+df2['x2'][idx]/2)*W),int((df2['y1'][idx]-df2['y2'][idx]/2)*H))\n",
    "        bottom_right = (int((df2['x1'][idx]-df2['x2'][idx]/2)*W),int((df2['y1'][idx]+df2['y2'][idx]/2)*H))\n",
    "        #top_left = (int(df2['x1'][idx]*W), int(df2['y1'][idx]*H))\n",
    "        #bottom_right = (int(df2['x2'][idx]*W), int(df2['y2'][idx]*H))\n",
    "        image = cv2.rectangle(image, top_left, bottom_right, [255, 0, 0], 2)\n",
    "\n",
    "        if float(c[0][i][0])>float(c[1][i][0]):\n",
    "            image = cv2.putText(image, 'Cat', (int(c[1][i]*W),int(c[1][i]*H)), font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "        else:\n",
    "            image = cv2.putText(image, 'Dog', (int(c[1][i]*W),int(c[1][i]*H)), font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "        i=i+1\n",
    "        #cv2.imshow('img',image)\n",
    "        cv2.imwrite(r'C:\\Users\\densh\\Desktop\\catdogs\\keras_try\\testing/'+title+'.jpg',image)\n",
    "        \n",
    "        print(idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python383jvsc74a57bd0184056108a3547f2193d2efa92fb0f74250f14e7b469048393661033d42ee1bf",
   "display_name": "Python 3.8.3 64-bit (conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}